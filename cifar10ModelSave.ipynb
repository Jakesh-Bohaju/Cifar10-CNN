{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('cifar10',as_supervised=True,\n",
    "    split = [tfds.Split.TRAIN.subsplit(tfds.percent[:80]),\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[80:90]),\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[90:]),],\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_OptionsDataset shapes: ((32, 32, 3), ()), types: (tf.uint8, tf.int64)>,\n",
       " <_OptionsDataset shapes: ((32, 32, 3), ()), types: (tf.uint8, tf.int64)>,\n",
       " <_OptionsDataset shapes: ((32, 32, 3), ()), types: (tf.uint8, tf.int64)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='cifar10',\n",
       "    version=1.0.2,\n",
       "    description='The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.',\n",
       "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=60000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 50000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
       "        author = {Alex Krizhevsky},\n",
       "        title = {Learning multiple layers of features from tiny images},\n",
       "        institution = {},\n",
       "        year = {2009}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[0]\n",
    "validation_dataset = dataset[1]\n",
    "test_dataset = dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    image = tf.dtypes.cast(image, tf.float32)\n",
    "    label = tf.dtypes.cast(label, tf.float32)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jakesh/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jakesh/anaconda3/envs/dl/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess)\n",
    "validation_dataset = validation_dataset.map(preprocess)\n",
    "test_dataset = test_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        initializer = tf.initializers.GlorotUniform(seed=123)\n",
    "        # Conv1\n",
    "        self.wc1 = tf.Variable(initializer([3, 3, 3, 10]), trainable=True, name='wc1')\n",
    "        \n",
    "        # Conv2\n",
    "        self.wc2 = tf.Variable(initializer([3, 3, 10, 20]), trainable=True, name='wc2')\n",
    "        \n",
    "        # Conv3\n",
    "        self.wc3 = tf.Variable(initializer([3, 3, 20, 40]), trainable=True, name='wc3')\n",
    "        \n",
    "        # Flatten\n",
    "        \n",
    "        # Dense\n",
    "        self.wd3 = tf.Variable(initializer([640, 280]), trainable=True)\n",
    "        self.wd4 = tf.Variable(initializer([280, 80]), trainable=True)        \n",
    "        self.wd5 = tf.Variable(initializer([80, 10]), trainable=True)\n",
    "        \n",
    "        self.bc1 = tf.Variable(tf.zeros([10]), dtype=tf.float32, trainable=True)\n",
    "        self.bc2 = tf.Variable(tf.zeros([20]), dtype=tf.float32, trainable=True)\n",
    "        self.bc3 = tf.Variable(tf.zeros([40]), dtype=tf.float32, trainable=True)\n",
    "        \n",
    "        self.bd3 = tf.Variable(tf.zeros([280]), dtype=tf.float32, trainable=True)\n",
    "        self.bd4 = tf.Variable(tf.zeros([80]), dtype=tf.float32, trainable=True)        \n",
    "        self.bd5 = tf.Variable(tf.zeros([10]), dtype=tf.float32, trainable=True)   \n",
    "    \n",
    "    def call(self, x):\n",
    "        # X = NHWC \n",
    "        # Conv1 + maxpool 2\n",
    "        x = tf.nn.conv2d(x, self.wc1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        x = tf.nn.bias_add(x, self.bc1)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        \n",
    "        # Conv2 + maxpool 2\n",
    "        x = tf.nn.conv2d(x, self.wc2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        x = tf.nn.bias_add(x, self.bc2)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        \n",
    "        # Conv3 + maxpool 3\n",
    "        x = tf.nn.conv2d(x, self.wc3, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        x = tf.nn.bias_add(x, self.bc3)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        \n",
    "        # Flattten out\n",
    "        # N X Number of Nodes\n",
    "        # Flatten()\n",
    "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "        \n",
    "        # Dense1\n",
    "        x = tf.matmul(x, self.wd3)\n",
    "        x = tf.nn.bias_add(x, self.bd3)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        \n",
    "        # Dense2\n",
    "        x = tf.matmul(x, self.wd4)\n",
    "        x = tf.nn.bias_add(x, self.bd4)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        \n",
    "        # Dense3\n",
    "        x = tf.matmul(x, self.wd5)\n",
    "        x = tf.nn.bias_add(x, self.bd5)\n",
    "#         x = tf.nn.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, labels, loss_fn, optimzer):\n",
    "    with tf.GradientTape() as t:\n",
    "        y_predicted = model(inputs, training=True)\n",
    "        current_loss = loss_fn(labels, y_predicted)\n",
    "\n",
    "    gradients = t.gradient(current_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_step(model, inputs, labels, loss_fn):\n",
    "    y_predicted = model(inputs, training=False)\n",
    "    current_loss = loss_fn(labels, y_predicted)\n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.randn(1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = tf.Variable(np.random.randn(3, 3, 3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tf.keras.metrics.Mean(name='loss')\n",
    "val_losses = tf.keras.metrics.Mean(name='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = f'./trainLog/{current_time}/logs'\n",
    "file_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = f'./ckpt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_checkpoint(manager):\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(f\"restored from {manager.latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Initializing from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch\n"
     ]
    }
   ],
   "source": [
    "check_for_checkpoint(manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Saved checkpoint for step 2: ./ckpt/ckpt-1\n",
      "tf.Tensor(1.3936287, shape=(), dtype=float32)\n",
      "epoch: 1\n",
      "Saved checkpoint for step 3: ./ckpt/ckpt-2\n",
      "tf.Tensor(1.0418618, shape=(), dtype=float32)\n",
      "epoch: 2\n",
      "Saved checkpoint for step 4: ./ckpt/ckpt-3\n",
      "tf.Tensor(0.87765324, shape=(), dtype=float32)\n",
      "epoch: 3\n",
      "Saved checkpoint for step 5: ./ckpt/ckpt-4\n",
      "tf.Tensor(0.7588274, shape=(), dtype=float32)\n",
      "epoch: 4\n",
      "Saved checkpoint for step 6: ./ckpt/ckpt-5\n",
      "tf.Tensor(0.6598256, shape=(), dtype=float32)\n",
      "epoch: 5\n",
      "Saved checkpoint for step 7: ./ckpt/ckpt-6\n",
      "tf.Tensor(0.58142835, shape=(), dtype=float32)\n",
      "epoch: 6\n",
      "Saved checkpoint for step 8: ./ckpt/ckpt-7\n",
      "tf.Tensor(0.5100259, shape=(), dtype=float32)\n",
      "epoch: 7\n",
      "Saved checkpoint for step 9: ./ckpt/ckpt-8\n",
      "tf.Tensor(0.44460648, shape=(), dtype=float32)\n",
      "epoch: 8\n",
      "Saved checkpoint for step 10: ./ckpt/ckpt-9\n",
      "tf.Tensor(0.38922107, shape=(), dtype=float32)\n",
      "epoch: 9\n",
      "Saved checkpoint for step 11: ./ckpt/ckpt-10\n",
      "tf.Tensor(0.33566892, shape=(), dtype=float32)\n",
      "epoch: 10\n",
      "Saved checkpoint for step 12: ./ckpt/ckpt-11\n",
      "tf.Tensor(0.2920303, shape=(), dtype=float32)\n",
      "epoch: 11\n",
      "Saved checkpoint for step 13: ./ckpt/ckpt-12\n",
      "tf.Tensor(0.26840845, shape=(), dtype=float32)\n",
      "epoch: 12\n",
      "Saved checkpoint for step 14: ./ckpt/ckpt-13\n",
      "tf.Tensor(0.24006514, shape=(), dtype=float32)\n",
      "epoch: 13\n",
      "Saved checkpoint for step 15: ./ckpt/ckpt-14\n",
      "tf.Tensor(0.22507422, shape=(), dtype=float32)\n",
      "epoch: 14\n",
      "Saved checkpoint for step 16: ./ckpt/ckpt-15\n",
      "tf.Tensor(0.20664214, shape=(), dtype=float32)\n",
      "epoch: 15\n",
      "Saved checkpoint for step 17: ./ckpt/ckpt-16\n",
      "tf.Tensor(0.19100735, shape=(), dtype=float32)\n",
      "epoch: 16\n",
      "Saved checkpoint for step 18: ./ckpt/ckpt-17\n",
      "tf.Tensor(0.17664851, shape=(), dtype=float32)\n",
      "epoch: 17\n",
      "Saved checkpoint for step 19: ./ckpt/ckpt-18\n",
      "tf.Tensor(0.16971761, shape=(), dtype=float32)\n",
      "epoch: 18\n",
      "Saved checkpoint for step 20: ./ckpt/ckpt-19\n",
      "tf.Tensor(0.16432846, shape=(), dtype=float32)\n",
      "epoch: 19\n",
      "Saved checkpoint for step 21: ./ckpt/ckpt-20\n",
      "tf.Tensor(0.14342873, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    ckpt.step.assign_add(1)\n",
    "\n",
    "    print(f'epoch: {epoch}')\n",
    "    losses.reset_states()\n",
    "    val_losses.reset_states()\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        loss = train_step(model, x_batch, y_batch, ce_loss, optimizer)\n",
    "        losses(loss)\n",
    "#         step += 1\n",
    "    \n",
    "    save_path = manager.save()\n",
    "    print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar('loss', losses.result(), step=epoch)\n",
    "        tf.summary.image('Input images', x_batch, step=epoch)\n",
    "\n",
    "    print(losses.result())\n",
    "        \n",
    "    for x_batch, y_batch in validation_dataset:\n",
    "        val_loss = valid_step(model, x_batch, y_batch, ce_loss)\n",
    "        val_losses(val_loss)\n",
    "    \n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar('val_loss', val_losses.result(), step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.train.list_variables(manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    predicted = model(inputs)\n",
    "    return tf.nn.softmax(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample, label in test_dataset.batch(2).take(1):\n",
    "#     predictions = predict(sample)\n",
    "#     print(tf.argmax(predictions, axis=1), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f'./models/'\n",
    "weights_path = os.path.join(model_dir, 'weights.h5')\n",
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}